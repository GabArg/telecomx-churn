{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d70b02",
   "metadata": {},
   "source": [
    "\n",
    "# Telecom X ‚Äì Parte 2: Predicci√≥n de Cancelaci√≥n (Churn)\n",
    "\n",
    "**Autor/a:** _Paula Almada_  \n",
    "\n",
    "**Fecha:** 2025-08-09\n",
    "\n",
    "Este notebook implementa un **pipeline de Machine Learning** completo para predecir **churn** en **Telecom X**, cumpliendo con todos los puntos indicados en la consigna:\n",
    "\n",
    "- **Preparaci√≥n de datos**: carga del archivo tratado (Parte 1), eliminaci√≥n de columnas irrelevantes, encoding y estandarizaci√≥n cuando corresponde.  \n",
    "- **Verificaci√≥n de proporci√≥n de churn** y **balanceo** de clases (**SMOTE**) aplicado correctamente s√≥lo sobre el conjunto de entrenamiento.  \n",
    "- **Correlaci√≥n y selecci√≥n** de variables, m√°s **an√°lisis dirigido** (Contract √ó Churn, TotalCharges √ó Churn).  \n",
    "- **Modelado** con al menos dos algoritmos (**Regresi√≥n Log√≠stica**, **Random Forest**) y uno adicional **KNN**.  \n",
    "- **Evaluaci√≥n** con Accuracy, Precision, Recall, F1, ROC-AUC y **matriz de confusi√≥n**, con comparaci√≥n cr√≠tica y an√°lisis de (under/over)fitting.  \n",
    "- **Interpretaci√≥n** de importancia de variables (coeficientes en Log√≠stica y feature_importances_ en Random Forest) y **conclusiones estrat√©gicas**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7310c9",
   "metadata": {},
   "source": [
    "## 1) üìö Librer√≠as y configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdd689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Si ejecutas en Colab, descomenta para instalar imbalanced-learn\n",
    "# !pip -q install imbalanced-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report, ConfusionMatrixDisplay, roc_auc_score, roc_curve, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# imblearn para SMOTE y Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (7,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d3002",
   "metadata": {},
   "source": [
    "\n",
    "## 2) üì• Carga del archivo tratado (Parte 1)\n",
    "\n",
    "> **Usa el mismo CSV** que limpiaste y estandarizaste en la Parte 1. Debe contener **s√≥lo columnas relevantes** y la columna objetivo **`Churn`** (0/1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01073d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Opci√≥n est√°ndar: el archivo ya est√° en /content ===\n",
    "# Sube 'datos_tratados.csv' a Colab (o monta Drive y ajusta la ruta).\n",
    "df = pd.read_csv('/content/datos_tratados.csv')\n",
    "\n",
    "print(f\"Filas: {df.shape[0]} | Columnas: {df.shape[1]}\")\n",
    "display(df.head())\n",
    "display(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee88ea1",
   "metadata": {},
   "source": [
    "\n",
    "## 3) üßπ Preparaci√≥n y verificaci√≥n de datos\n",
    "\n",
    "**3.1 Elimina columnas irrelevantes:** IDs y columnas anidadas que no aportan al modelo.  \n",
    "**3.2 Define `X` e `y` y detecta tipos (num√©ricas/categ√≥ricas).  \n",
    "**3.3 Proporci√≥n de churn y verificaci√≥n de balanceo.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cbafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3.1 Columnas a eliminar (ajusta si tu dataset no las tiene)\n",
    "cols_to_drop = [\"customerID\", \"customer\", \"phone\", \"internet\", \"account\"]\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# 3.2 Definir target y features\n",
    "target_col = \"Churn\"\n",
    "assert target_col in df.columns, \"No se encontr√≥ la columna 'Churn' en el dataset.\"\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col].astype(int)\n",
    "\n",
    "# Tipos\n",
    "num_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "print(\"Columnas num√©ricas:\", len(num_cols))\n",
    "print(\"Columnas categ√≥ricas:\", len(cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3.3 Proporci√≥n de churn y chequeo de desbalance\n",
    "prop = y.value_counts(normalize=True).rename({0:'No Churn', 1:'Churn'})\n",
    "display(prop)\n",
    "prop.plot(kind='bar', title='Proporci√≥n de clases (Churn)'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0843b",
   "metadata": {},
   "source": [
    "\n",
    "## 4) ‚úÇÔ∏è Split y preprocesamiento\n",
    "\n",
    "- **Split**: 70/30 estratificado.  \n",
    "- **Preprocesamiento**:  \n",
    "  - **One-Hot Encoding** para categ√≥ricas.  \n",
    "  - **Estandarizaci√≥n** para num√©ricas **s√≥lo** en modelos que lo requieren (Log√≠stica, KNN).  \n",
    "  - **√Årboles/Random Forest** no requieren escalado, pero lo incluimos sin perjuicio.  \n",
    "- **SMOTE**: aplicado **s√≥lo sobre el set de entrenamiento** dentro del pipeline para evitar fuga de informaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbfb386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n",
    "print(\"Proporci√≥n churn (train/test):\", round(y_train.mean(),3), \"/\", round(y_test.mean(),3))\n",
    "\n",
    "# Preprocesadores\n",
    "preprocess_scaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "preprocess_noscale = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0f26e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) üìä Correlaci√≥n y an√°lisis dirigido\n",
    "\n",
    "**5.1 Matriz de correlaci√≥n** (num√©ricas + `Churn`).  \n",
    "**5.2 Contract √ó Churn** (tasa de churn por tipo de contrato).  \n",
    "**5.3 TotalCharges √ó Churn** (boxplot por clase).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.1 Matriz de correlaci√≥n\n",
    "if len(num_cols) > 0:\n",
    "    corr = df[num_cols + [target_col]].corr()\n",
    "    plt.figure()\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(\"Matriz de correlaci√≥n (num√©ricas)\")\n",
    "    plt.show()\n",
    "\n",
    "    corr_target = corr[target_col].drop(target_col).sort_values(ascending=False)\n",
    "    display(corr_target.head(10))\n",
    "else:\n",
    "    print(\"No se detectaron columnas num√©ricas para correlaci√≥n.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6726c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.2 Contract √ó Churn\n",
    "if 'Contract' in df.columns:\n",
    "    rate_by_contract = df.groupby('Contract')[target_col].mean().sort_values(ascending=False)\n",
    "    display(rate_by_contract)\n",
    "    rate_by_contract.plot(kind='bar', title='Tasa de churn por tipo de contrato'); plt.show()\n",
    "else:\n",
    "    print(\"No se encontr√≥ la columna 'Contract' para el an√°lisis dirigido.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.3 TotalCharges √ó Churn (boxplot)\n",
    "if 'TotalCharges' in df.columns:\n",
    "    plt.figure()\n",
    "    sns.boxplot(data=df, x=target_col, y='TotalCharges')\n",
    "    plt.title(\"TotalCharges √ó Churn (boxplot)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se encontr√≥ 'TotalCharges' para el an√°lisis dirigido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920cb8b2",
   "metadata": {},
   "source": [
    "\n",
    "## 6) ü§ñ Modelado con balanceo de clases (SMOTE)\n",
    "\n",
    "Entrenamos **tres** modelos:  \n",
    "- **Regresi√≥n Log√≠stica** (requiere escalado).  \n",
    "- **K-Nearest Neighbors (KNN)** (requiere escalado).  \n",
    "- **Random Forest** (√°rboles, no requiere escalado).  \n",
    "\n",
    "> SMOTE se aplica **dentro del pipeline**, **s√≥lo al entrenamiento**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92588ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(name, clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        proba = clf.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        # Para modelos sin predict_proba (no aplica aqu√≠), usamos decision_function si existe\n",
    "        proba = None\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    prec = precision_score(y_test, preds)\n",
    "    rec = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    rocauc = roc_auc_score(y_test, proba) if proba is not None else np.nan\n",
    "\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f} | ROC-AUC: {rocauc:.3f}\")\n",
    "    print(classification_report(y_test, preds, digits=3))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, preds)\n",
    "    plt.title(f\"Matriz de confusi√≥n - {name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC\n",
    "    if proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={rocauc:.3f})\")\n",
    "        plt.plot([0,1], [0,1], linestyle='--')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"Curva ROC - {name}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return {\"name\": name, \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"rocauc\": rocauc}\n",
    "\n",
    "# Pipelines con SMOTE\n",
    "log_reg = ImbPipeline(steps=[\n",
    "    (\"prep\", preprocess_scaled),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "knn = ImbPipeline(steps=[\n",
    "    (\"prep\", preprocess_scaled),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", KNeighborsClassifier(n_neighbors=15))\n",
    "])\n",
    "\n",
    "rf = ImbPipeline(steps=[\n",
    "    (\"prep\", preprocess_noscale),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_model(\"Logistic Regression (SMOTE)\", log_reg, X_train, y_train, X_test, y_test))\n",
    "results.append(evaluate_model(\"KNN (SMOTE)\", knn, X_train, y_train, X_test, y_test))\n",
    "results.append(evaluate_model(\"Random Forest (SMOTE)\", rf, X_train, y_train, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14b7ab",
   "metadata": {},
   "source": [
    "\n",
    "## 7) üìà Importancia de variables e interpretaci√≥n\n",
    "\n",
    "- **Random Forest:** `feature_importances_` (reducci√≥n de impureza).  \n",
    "- **Regresi√≥n Log√≠stica:** coeficientes (signo y magnitud).  \n",
    "> Para interpretar correctamente, usamos los **nombres de las columnas transformadas** tras el One-Hot Encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importancia Random Forest\n",
    "ohe_rf = rf.named_steps['prep'].named_transformers_['cat']\n",
    "cat_features_rf = ohe_rf.get_feature_names_out(input_features=rf.named_steps['prep'].transformers_[1][2]).tolist() if len(rf.named_steps['prep'].transformers_[1][2])>0 else []\n",
    "num_features_rf = rf.named_steps['prep'].transformers_[0][2]\n",
    "feature_names_rf = list(num_features_rf) + cat_features_rf\n",
    "\n",
    "rf_importances = rf.named_steps['model'].feature_importances_\n",
    "imp_df = pd.DataFrame({'Feature': feature_names_rf, 'Importance': rf_importances}).sort_values('Importance', ascending=False)\n",
    "display(imp_df.head(20))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=imp_df.head(20), x='Importance', y='Feature')\n",
    "plt.title(\"Top 20 variables m√°s importantes - Random Forest\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4447a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Coeficientes Regresi√≥n Log√≠stica\n",
    "ohe_lr = log_reg.named_steps['prep'].named_transformers_['cat']\n",
    "cat_features_lr = ohe_lr.get_feature_names_out(input_features=log_reg.named_steps['prep'].transformers_[1][2]).tolist() if len(log_reg.named_steps['prep'].transformers_[1][2])>0 else []\n",
    "num_features_lr = log_reg.named_steps['prep'].transformers_[0][2]\n",
    "feature_names_lr = list(num_features_lr) + cat_features_lr\n",
    "\n",
    "coefs = log_reg.named_steps['model'].coef_[0]\n",
    "coef_df = pd.DataFrame({'Feature': feature_names_lr, 'Coef': coefs}).sort_values('Coef', ascending=False)\n",
    "display(coef_df.head(15))\n",
    "display(coef_df.tail(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d0b444",
   "metadata": {},
   "source": [
    "\n",
    "## 8) üìä Comparaci√≥n de modelos y (under/over)fitting\n",
    "\n",
    "Completa con tus resultados (copiando m√©tricas de la secci√≥n de evaluaci√≥n). Reflexiona:\n",
    "\n",
    "- **Mejor desempe√±o** (¬øcu√°l y por qu√©?).  \n",
    "- **Overfitting** (alto rendimiento en train y pobre en test): posibles causas y mitigaciones.  \n",
    "- **Underfitting** (bajo rendimiento general): acciones para mejorar.  \n",
    "\n",
    "> Pistas:  \n",
    "> - **Random Forest** suele rendir bien y ser robusto.  \n",
    "> - **KNN** es sensible a la escala y al valor de `k`, y puede sufrir con muchas variables dummificadas.  \n",
    "> - **Log√≠stica** es interpretable; revisa coeficientes y supuestos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31977856",
   "metadata": {},
   "source": [
    "\n",
    "## 9) üìù Conclusiones y recomendaciones\n",
    "\n",
    "**Factores que m√°s influyen en la cancelaci√≥n**  \n",
    "- Resume las variables top seg√∫n **Random Forest** y los **coeficientes** de Log√≠stica (direcci√≥n + magnitud).\n",
    "\n",
    "**Estrategias de retenci√≥n**  \n",
    "- Clientes con contrato **Month-to-month** y cargos mensuales altos.  \n",
    "- Usuarios sin **TechSupport/OnlineSecurity**.  \n",
    "- Ajusta campa√±as y ofertas seg√∫n segmentos de riesgo.\n",
    "\n",
    "**Siguientes pasos (opcional)**  \n",
    "- B√∫squeda de hiperpar√°metros (Grid/Random Search).  \n",
    "- Probar **XGBoost/LightGBM**.  \n",
    "- Calibraci√≥n de probabilidades y ajuste del **umbral** de clasificaci√≥n seg√∫n costo/beneficio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b105e77",
   "metadata": {},
   "source": [
    "### Extra (opcional) ‚Äì B√∫squeda de hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e90902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# rf_search = ImbPipeline(steps=[('prep', preprocess_noscale), ('smote', SMOTE(random_state=42)), ('model', RandomForestClassifier(random_state=42, n_jobs=-1))])\n",
    "# param_dist = {\n",
    "#     'model__n_estimators': [200, 300, 500],\n",
    "#     'model__max_depth': [None, 6, 10, 15],\n",
    "#     'model__min_samples_split': [2, 5, 10],\n",
    "#     'model__min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "# rs = RandomizedSearchCV(rf_search, param_distributions=param_dist, n_iter=10, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42)\n",
    "# rs.fit(X_train, y_train)\n",
    "# print(\"Mejores par√°metros:\", rs.best_params_)\n",
    "# print(\"Mejor ROC-AUC (CV):\", rs.best_score_)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
