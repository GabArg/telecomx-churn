{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ff8466",
   "metadata": {},
   "source": [
    "\n",
    "# Telecom X – Parte 2: Predicción de Cancelación (Churn)\n",
    "\n",
    "**Autor/a:** _Paula Almada_  \n",
    "**Fecha:** 2025-08-08\n",
    "\n",
    "En este notebook construimos un pipeline de **Machine Learning** para predecir la probabilidad de **cancelación (churn)** en clientes de **Telecom X**.\n",
    "\n",
    "**Objetivos:**  \n",
    "- Preparar y preprocesar los datos (tratamiento, codificación, normalización).  \n",
    "- Realizar análisis de correlación y selección de variables.  \n",
    "- Entrenar **dos o más** modelos de clasificación.  \n",
    "- Evaluar el rendimiento con métricas (Accuracy, Precision, Recall, F1, ROC-AUC).  \n",
    "- Interpretar resultados (importancia de variables) y presentar **conclusiones estratégicas**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d7f80",
   "metadata": {},
   "source": [
    "## 1) 📚 Librerías y configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manejo de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, roc_auc_score\n",
    "\n",
    "# Estilo y opciones\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (7,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001f6d0",
   "metadata": {},
   "source": [
    "\n",
    "## 2) 📥 Carga y exploración inicial\n",
    "\n",
    "> **Opciones de carga:**\n",
    "> - **A.** Subir archivo `datos_tratados.csv` al entorno de Colab y leerlo desde `/content/datos_tratados.csv`  \n",
    "> - **B.** Montar Google Drive y leer desde tu carpeta\n",
    "\n",
    "> **Nota:** Este notebook asume un archivo llamado **`datos_tratados.csv`** con una columna objetivo **`Churn`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Opción A: subir archivo manualmente ===\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # luego: df = pd.read_csv('datos_tratados.csv')\n",
    "\n",
    "# === Opción B: desde Google Drive ===\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/ruta/datos_tratados.csv')\n",
    "\n",
    "# === Lectura directa si ya está en /content ===\n",
    "# (Asegúrate de tener 'datos_tratados.csv' en /content)\n",
    "df = pd.read_csv('/content/datos_tratados.csv')\n",
    "\n",
    "print(f\"Filas: {df.shape[0]}  |  Columnas: {df.shape[1]}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Información general y nulos\n",
    "display(df.info())\n",
    "nulls = df.isna().sum().sort_values(ascending=False)\n",
    "display(nulls.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8876b65",
   "metadata": {},
   "source": [
    "\n",
    "## 3) 🧹 Preprocesamiento\n",
    "\n",
    "**Pasos:**  \n",
    "1. Eliminar columnas no informativas (IDs) y anidadas en forma de diccionario del EDA previo (si existieran).  \n",
    "2. Separar variables numéricas y categóricas.  \n",
    "3. **One-Hot Encoding** para categóricas y **escalado** para numéricas.  \n",
    "4. **Train/Test split** estratificado por `Churn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Eliminar columnas no útiles (ajusta si tu dataset cambia)\n",
    "cols_to_drop = [\"customerID\", \"customer\", \"phone\", \"internet\", \"account\"]\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# 2) Definir X (features) e y (target)\n",
    "target_col = \"Churn\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# 3) Tipos de variables\n",
    "num_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "\n",
    "print(\"Numéricas:\", len(num_cols), \"| Categóricas:\", len(cat_cols))\n",
    "\n",
    "# 4) Split estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n",
    "print(\"Proporción churn (train/test):\", round(y_train.mean(),3), \"/\", round(y_test.mean(),3))\n",
    "\n",
    "# 5) Preprocesamiento\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2aefa9",
   "metadata": {},
   "source": [
    "\n",
    "## 4) 📊 Análisis de correlación (numéricas)\n",
    "\n",
    "> La correlación de Pearson se calcula entre variables **numéricas** y la variable objetivo `Churn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Matriz de correlación (solo columnas numéricas + target)\n",
    "if len(num_cols) > 0:\n",
    "    corr = df[num_cols + [target_col]].corr()\n",
    "    # Heatmap\n",
    "    plt.figure()\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(\"Matriz de correlación (numéricas)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Orden de correlación con respecto a Churn\n",
    "    corr_target = corr[target_col].drop(target_col).sort_values(ascending=False)\n",
    "    display(corr_target.head(10))\n",
    "else:\n",
    "    print(\"No se detectaron columnas numéricas para correlación.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a319bb0",
   "metadata": {},
   "source": [
    "\n",
    "## 5) 🤖 Modelos de clasificación\n",
    "\n",
    "Entrenamos al menos **dos modelos**:  \n",
    "- **Regresión Logística** (baseline interpretable)  \n",
    "- **Random Forest** (árboles en conjunto, captura relaciones no lineales)\n",
    "\n",
    "> Métricas clave: **Accuracy, Precision, Recall, F1, ROC-AUC** y **Matriz de confusión**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Modelo 1: Regresión Logística ===\n",
    "log_reg = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, n_jobs=None))\n",
    "])\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "pred_lr = log_reg.predict(X_test)\n",
    "proba_lr = log_reg.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"=== Regresión Logística ===\")\n",
    "print(classification_report(y_test, pred_lr, digits=3))\n",
    "print(\"ROC-AUC:\", round(roc_auc_score(y_test, proba_lr), 3))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, pred_lr)\n",
    "plt.title(\"Matriz de confusión - Logística\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Modelo 2: Random Forest ===\n",
    "rf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=None, min_samples_split=2,\n",
    "        min_samples_leaf=1, random_state=42, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "proba_rf = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"=== Random Forest ===\")\n",
    "print(classification_report(y_test, pred_rf, digits=3))\n",
    "print(\"ROC-AUC:\", round(roc_auc_score(y_test, proba_rf), 3))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, pred_rf)\n",
    "plt.title(\"Matriz de confusión - Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb72848",
   "metadata": {},
   "source": [
    "\n",
    "## 6) 📈 Importancia de variables\n",
    "\n",
    "- **Random Forest:** `feature_importances_` (importancia basada en reducción de impureza).  \n",
    "- **Regresión Logística:** coeficientes para interpretar la dirección del efecto (opcional).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extraer nombres de features transformadas\n",
    "# Nota: tras fit, podemos acceder al OneHotEncoder dentro del ColumnTransformer\n",
    "ohe = rf.named_steps['prep'].named_transformers_['cat']\n",
    "cat_feature_names = []\n",
    "if len(cat_cols) > 0:\n",
    "    cat_feature_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "\n",
    "feature_names = num_cols + cat_feature_names\n",
    "importances = rf.named_steps['model'].feature_importances_\n",
    "\n",
    "feat_imp = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feat_imp = feat_imp.sort_values('Importance', ascending=False)\n",
    "\n",
    "top_k = 20\n",
    "display(feat_imp.head(top_k))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=feat_imp.head(top_k), x='Importance', y='Feature')\n",
    "plt.title(f\"Top {top_k} variables más importantes (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14d52d",
   "metadata": {},
   "source": [
    "### Opcional: Coeficientes de la Regresión Logística (interpretabilidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e0bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtenemos el mismo orden de columnas que ve el modelo logístico\n",
    "log_ohe = log_reg.named_steps['prep'].named_transformers_['cat']\n",
    "log_cat_feature_names = []\n",
    "if len(cat_cols) > 0:\n",
    "    log_cat_feature_names = log_ohe.get_feature_names_out(cat_cols).tolist()\n",
    "\n",
    "log_feature_names = num_cols + log_cat_feature_names\n",
    "coefs = log_reg.named_steps['model'].coef_[0]\n",
    "coef_df = pd.DataFrame({'Feature': log_feature_names, 'Coef': coefs}).sort_values('Coef', ascending=False)\n",
    "display(coef_df.head(15))\n",
    "display(coef_df.tail(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a82bcd",
   "metadata": {},
   "source": [
    "\n",
    "## 7) 📝 Conclusiones y recomendaciones\n",
    "\n",
    "**Resumen de desempeño**\n",
    "- ROC-AUC **Random Forest**: _(completar)_\n",
    "- ROC-AUC **Logística**: _(completar)_\n",
    "- Trade-off entre **Recall** (detectar churners) y **Precision**.\n",
    "\n",
    "**Factores clave en churn (top importancia)**\n",
    "- Variables más influyentes según RF y signos de coeficientes en Logística.\n",
    "\n",
    "**Recomendaciones estratégicas**\n",
    "- Segmentar y priorizar clientes con mayor riesgo (alto `MonthlyCharges`, contratos `Month-to-month`, sin `TechSupport`, etc. **[ajustar según resultados reales]**).\n",
    "- Probar campañas de **retención** y **cross-sell** específicas.\n",
    "- Monitoreo continuo con umbral de probabilidad ajustado al costo-beneficio.\n",
    "\n",
    "---\n",
    "\n",
    "> _Espacio para observaciones adicionales del/la analista._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95867c0",
   "metadata": {},
   "source": [
    "### Extra (opcional): Búsqueda de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# param_dist = {\n",
    "#     'model__n_estimators': [200, 300, 500],\n",
    "#     'model__max_depth': [None, 6, 10, 15],\n",
    "#     'model__min_samples_split': [2, 5, 10],\n",
    "#     'model__min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "# rf_search = Pipeline(steps=[('prep', preprocess), ('model', RandomForestClassifier(random_state=42, n_jobs=-1))])\n",
    "# rs = RandomizedSearchCV(rf_search, param_distributions=param_dist, n_iter=10, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42)\n",
    "# rs.fit(X_train, y_train)\n",
    "# print(\"Mejores parámetros:\", rs.best_params_)\n",
    "# print(\"Mejor ROC-AUC (CV):\", rs.best_score_)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
